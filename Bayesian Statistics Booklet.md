**Bayesian Statistics Booklet for Master Degree Students**

**Introduction**

This booklet serves as a concise guide for master degree students who are embarking on a journey through Bayesian statistics. It covers the foundational knowledge and key theories outlined in the syllabus, aiming to provide a solid foundation for further study and research in this exciting field.

**Chapter 1: Probability Basics**

- **Basic Components of Probability**:
  - *Events*: Outcomes of a random experiment.
  - *Probability*: A numerical measure of the likelihood of an event occurring.
  - *Sample Space*: The set of all possible outcomes of a random experiment.

- **Rules of Probability**:
  - *Addition Rule*: For mutually exclusive events, the probability of either event occurring is the sum of their individual probabilities.
  - *Multiplication Rule*: The probability of two events occurring consecutively is the product of their individual probabilities.

**Chapter 2: Joint Probabilities and Factorization**

- **Joint Probabilities**: The probability of two or more events occurring simultaneously.
- **Factorization**: Expressing a joint probability as the product of conditional probabilities. This is crucial for understanding Bayesian reasoning.

**Chapter 3: Distributions**

- **Basic Descriptions of Distributions**:
  - *Discrete Distributions*: Distributions with a countable number of possible outcomes, such as the binomial and Poisson distributions.
  - *Continuous Distributions*: Distributions with an infinite number of possible outcomes, such as the normal and exponential distributions.
  
  - *Properties*: Mean, variance, median, and other summary statistics used to describe distributions.

**Chapter 4: Estimation Procedures**

- **Method of Moments (MoM)**: A technique for estimating parameters of a distribution by equating sample moments to population moments.
- **Maximum Likelihood Estimation (MLE)**: A method for estimating parameters that maximize the likelihood function, i.e., the probability of observing the data given the parameters.

**Chapter 5: Bayes' Rule and Bayesian Inference**

- **Bayes' Rule**: The cornerstone of Bayesian statistics, it relates the conditional probabilities of two events.
  - Formula: $ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $
  - Interpretation: The probability of A given B is proportional to the probability of B given A, multiplied by the prior probability of A, and normalized by the probability of B.

- **Prior Distribution**: A probability distribution that represents our beliefs about a parameter before observing the data.
  - *Informed Prior*: Based on previous knowledge or expert opinion.
  - *Vague Prior*: A non-informative prior that has minimal influence on the posterior distribution.

**Chapter 6: Decision Making with Loss Functions**

- **Loss Functions**: A mechanism for quantifying the cost or penalty associated with different decisions or predictions.
- **Bayesian Decision Theory**: Uses loss functions and posterior distributions to make optimal decisions under uncertainty.

**Chapter 7: Comparing Priors and Posteriors**

- **Difference Between Informed and Vague Priors**:
  - *Informed Priors*: Incorporate prior knowledge, leading to a more focused posterior distribution.
  - *Vague Priors*: Have minimal influence, allowing the data to dominate the posterior distribution.

**Conclusion**

Bayesian statistics offers a powerful framework for making inferences under uncertainty. By understanding the basic components of probability, joint probabilities, distributions, estimation procedures, and Bayes' rule, master degree students can lay a solid foundation for further exploration of this fascinating field. The concept of priors and their influence on posterior distributions is particularly important in Bayesian inference, as it allows us to incorporate prior knowledge into our analyses. With this booklet as a reference, students will be well-equipped to embark on their journey through the world of Bayesian statistics.



